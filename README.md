**Este Notebook foi usado para uma demonstra√ß√£o interna com o time do AI Studio para mostrar um fluxo de trabalho comum em DS ü§ó**

# Let's bora? Explica√ß√µes iniciais

### O que √© um classificador de Spam? ü§î

Um classificador de spam √© como um detetive digital esperto que examina suas mensagens e decide quais s√£o boas e quais s√£o "lixo". Imagine que ele tem uma lupa m√°gica que pode ver padr√µes suspeitos nas palavras, como "dinheiro f√°cil" ou "clique aqui". Quando ele encontra essas pistas, coloca a mensagem na "lixeira" de spam, protegendo sua caixa de entrada de ser invadida por ofertas de pr√≠ncipes nigerianos ou de produtos milagrosos que ningu√©m pediu!

## Sim, o HuggingFace √© uma queen! üòç

### Da onde vem nosso Dataset? ü§î

Vem da biblioteca Datasets, tamb√©m do HuggingFace! √â uma ferramenta para facilitar o acesso e o uso de conjuntos de dados em projetos de machine learning. Ela permite carregar, processar e manipular dados, suportando uma ampla variedade de formatos e fontes de dados.

### Quem √© Transformers? ü§î

A biblioteca Transformers do HuggingFace √© uma ferramenta para trabalhar com modelos de linguagem natural (NLP) baseados na arquitetura Transformers, como BERT, GPT, e T5. Ela fornece uma interface f√°cil de usar para carregar, treinar e utilizar esses modelos para tarefas como tradu√ß√£o, resumo de texto, classifica√ß√£o e gera√ß√£o de texto!

### Por que precisamos do Accelerate se n√£o importamos ele? ü§î

O Accelerate √© uma biblioteca TAMB√âM da Hugging Face que sereve para facilitar o uso de m√∫ltiplos dispositivos (como GPUs e TPUs) ao treinar e inferir modelos de aprendizado profundo. Mesmo que voc√™ n√£o importe diretamente no seu c√≥digo, voc√™ pode estar utilizando funcionalidades ou bibliotecas que dependem do Accelerate para melhorar a efici√™ncia e o desempenho.

## BERT ü§ñ

BERT (Bidirectional Encoder Representations from Transformers) √© um modelo de linguagem natural desenvolvido pelo Google que revolucionou o processamento de linguagem natural (NLP) ao entender o contexto de palavras em uma frase de forma bidirecional, ou seja, analisando as palavras √† esquerda e √† direita de cada palavra-alvo ao mesmo tempo.

O `bert-base-uncased` √© uma vers√£o do modelo BERT com 12 camadas (ou "transformers") e 110 milh√µes de par√¢metros. Esses par√¢metros s√£o como "neur√¥nios" em uma rede neural que aprendem a representar o significado das palavras.

# M√£os na massa (ou no AIS? ü§î)
Explore o Jupyter Notebook em [SpamDetectionBERT](SpamDetetcionBERT.ipynb)
